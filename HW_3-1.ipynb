{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1sQ9BGHqBmZ"
   },
   "source": [
    "# Assignment 3 #\n",
    "### Due: Monday, October 9th to be submitted via Canvas by 11:59 pm ###\n",
    "### Total points: **90** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y86kiHXJYHCy"
   },
   "source": [
    "Your homework should be written in a python notebook. If you prefer, you can work in groups of two. **Please note that only one student per team needs to submit the assignment on Canvas but make sure to include both students' names, UT EIDs and the homework group.**  \n",
    "\n",
    "For any question that requires a handwritten solution, you may upload scanned images of your solution in the notebook or attach them to the assignment . You may write your solution using markdown as well.\n",
    "\n",
    "Please make sure your code runs and the graphs and figures are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "\n",
    "1. Homework Group -\n",
    "2. Student Names -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_PNd32DnKcu"
   },
   "source": [
    "## Q1. (30 points) - Comparing MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YTDMq9RnKc0"
   },
   "source": [
    "In this problem, we will be comparing different MLP configurations on the California Housing dataset and the Diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5Ai294RnKc0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQkJF0MvnKc0"
   },
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFT4Ji-tnKc0"
   },
   "outputs": [],
   "source": [
    "# Load the Calinifornia Housing dataset and do train/val/test split\n",
    "california_housing = fetch_california_housing()\n",
    "housing_X, housing_y = california_housing['data'], california_housing['target']\n",
    "housing_X_train, housing_X_tmp, housing_y_train, housing_y_tmp = train_test_split(housing_X, housing_y, test_size=0.4, random_state=seed)\n",
    "housing_X_val, housing_X_test, housing_y_val, housing_y_test = train_test_split(housing_X_tmp, housing_y_tmp, test_size=0.5, random_state=seed)\n",
    "\n",
    "housing_scaler = StandardScaler()\n",
    "housing_X_train = housing_scaler.fit_transform(housing_X_train)\n",
    "housing_X_val = housing_scaler.transform(housing_X_val)\n",
    "housing_X_test = housing_scaler.transform(housing_X_test)\n",
    "print(california_housing['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TXe9gCsnKc0"
   },
   "outputs": [],
   "source": [
    "# Load the Diabetes dataset and do train/val/test split\n",
    "diabetes = load_diabetes()\n",
    "diabetes_X, diabetes_y = diabetes['data'], diabetes['target']\n",
    "\n",
    "diabetes_X_train, diabetes_X_tmp, diabetes_y_train, diabetes_y_tmp = train_test_split(diabetes_X, diabetes_y, test_size=0.4, random_state=seed)\n",
    "diabetes_X_val, diabetes_X_test, diabetes_y_val, diabetes_y_test = train_test_split(diabetes_X_tmp, diabetes_y_tmp, test_size=0.5, random_state=seed)\n",
    "\n",
    "diabetes_scaler = StandardScaler()\n",
    "diabetes_X_train = diabetes_scaler.fit_transform(diabetes_X_train)\n",
    "diabetes_X_val = diabetes_scaler.transform(diabetes_X_val)\n",
    "diabetes_X_test = diabetes_scaler.transform(diabetes_X_test)\n",
    "print(diabetes['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBtypTpYnKc0"
   },
   "source": [
    "**Part 1** **(10 pts)**. Write the training and evaluation functions of the MLP. Use the default parameter values of sklearn.neural_network.MLPRegressor except:\n",
    "\n",
    "*   **hidden_layer_size**: given by train_mlp parameter\n",
    "*   **learning_rate_init**: given by a list of search space\n",
    "*   **random_state**: given by train_mlp parameter\n",
    "*   **max_iter**: fix at 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vNvRJXBnKc0"
   },
   "outputs": [],
   "source": [
    "def train_mlp(hidden_layer_size, X_train, y_train, X_val, y_val, seed):\n",
    "    learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "    best_r2 = -np.inf # Determine the best model based on the R2 score on validation set # TO HIDE\n",
    "    for lr in learning_rate_list:\n",
    "        # Initialize MLPRegressor\n",
    "\n",
    "        # Fit the MLPRegressor to training data\n",
    "\n",
    "        # Predict and evaluate on train and validation data\n",
    "        mse_train, r2_train = eval_model(model, X_train, y_train)\n",
    "        mse_val, r2_val = eval_model(model, X_val, y_val)\n",
    "        print(f\"Learning rate: {lr} MSE train: {mse_train} R2 train: {r2_train} MSE val: {mse_val} R2 val: {r2_val}\")\n",
    "\n",
    "        # Record the best model according to R2 score on validation set\n",
    "        if r2_val > best_r2:\n",
    "\n",
    "    return best_model, best_lr, best_r2\n",
    "\n",
    "def eval_model(model, X, y):\n",
    "    # Predict and evaluate\n",
    "    return mse, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vlQE-atnKc1"
   },
   "source": [
    "**Part 2** **(5 pts)**.\n",
    "Train two MLPs on the **housing dataset** with the following two different hidden layer size configurations and show their **MSE** and **R2 score** on the **test set**.\n",
    "*   (8)\n",
    "*   (64, 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9jDom_VnKc1"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "### END CODE ###\n",
    "print(f\"Test MSE: {mse_test} Test R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNG_qR4DnKc1"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "### END CODE ###\n",
    "print(f\"Test MSE: {mse_test} Test R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQX-c2vEnKc1"
   },
   "source": [
    "**Part 3** **(5 pts)**. Train a **linear regression model** on the **housing dataset** and show their MSE and R2 scores on the test set. How do the performances of the two MLPs and the linear regression model compare, and what do you think causes the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oPAAqndnKc1"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "### END CODE ###\n",
    "print(f\"Test MSE: {mse_test} Test R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HMizilSnKc1"
   },
   "source": [
    "**Part 4** **(5 pts)**. Now, train two MLPs on the **diabetes dataset** with the following two different hidden layer size configurations and show their **MSE** and **R2 score** on the **test set**.\n",
    "*   (8)\n",
    "*   (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yqekb_NnKc1"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "### END CODE ###\n",
    "print(f\"Test MSE: {mse_test} Test R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnmZwekdnKc1"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "### END CODE ###\n",
    "print(f\"Test MSE: {mse_test} Test R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrrBLzeqnKc1"
   },
   "source": [
    "**Part 5** **(5 pts)**. Train another linear regression model on the **diabetes dataset** and show its **MSE** and **R2 score** on the **test set**. How do the performances of two MLPs and the linear regression model compare? Is the performance order on the diabetes dataset the same as the one on the housing dataset? If not, what causes the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFG9SBPNnKc1"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "### END CODE ###\n",
    "print(f\"Test MSE: {mse_test} Test R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pb9fxiUfPsv"
   },
   "source": [
    "#Q2. (20 points) Tensorflow Playground\n",
    "In this question, you will be playing with [Tensorflow Playground](https://playground.tensorflow.org).\n",
    "\n",
    "\n",
    "Select **Classification** as the Problem Type. Among the four datasets shown in DATA, please select the top left dataset.\n",
    "\n",
    "Use the following settings as the DEFAULT settings for all subquestions:\n",
    "\n",
    "\n",
    "*   Learning rate = 0.03\n",
    "*   Activation = Tanh\n",
    "*   Regularization = None\n",
    "*   Ratio of training to test data = 50%\n",
    "*   Noise = 0\n",
    "*   Batch Size = 30\n",
    "*   input as  ùëã1  with  ùëã2\n",
    "*   One hidden layer with 4 neurons\n",
    "\n",
    "a) **(4 pts)** Use the DEFAULT setting and run two experiments -\n",
    "\n",
    "1.   Using Tanh as the activation function\n",
    "2.   Using the Linear activation function.\n",
    "\n",
    "Report the train, test losses for both at the end of 1000 epochs. What qualitative difference do you observe in the decision boundaries obtained? What do you think is the reason for this?\n",
    "\n",
    "We will now study the effect of certain variations in the network structure or training process, keeping all other aspects the same as in the DEFAULT setting specified above, with Tanh as the activation.\n",
    "\n",
    "b) **(4 pts)** Effect of number of hidden units: Keep other settings the same as in DEFAULT.\n",
    "\n",
    "\n",
    "1.   Report the training loss and test loss at the end of 1000 epochs using 2 neurons and 8 neurons in the hidden layer.\n",
    "2.   What do you observe in terms of the decision boundary obtained as the number of neurons increases? What do you think is the reason for this?\n",
    "\n",
    "c) **(4 pts)** Effect of Learning rate and number of epochs: Keep other settings the same as in DEFAULT.\n",
    "\n",
    "1.   For learning rate 10, 1, 0.1, 0.01 and 0.0001, report the train, test losses at the end of 100 epochs, 500 epochs and 1000 epochs respectively.\n",
    "2.   What do you observe from the change of loss vs learning rate, and the change of loss vs epoch numbers? Also report your observations on the training and test loss curve (observe if you see noise for certain learning rates and reason why this is happening).\n",
    "\n",
    "\n",
    "\n",
    "d) **(4 pts)** Effect of the number of layers:\n",
    "\n",
    "1.   Change your activation to ReLU and use a single hidden layer with 4 neurons and then add another hidden layer with 3 neurons and train both your models for 1000 epochs.\n",
    "2.   Comment on your final models and decision boundaries and observe your training and test loss curves as well.\n",
    "\n",
    "\n",
    "e) **(4 pts)** Use the DEFAULT setting. Play around with any hyperparameters, network architectures or input features (such as  sin(ùëã1),ùëã21  etc.), and report the best train and test loss you obtain (test loss should be no greater than 0.06). Attach the screenshot showing your full network, output and the parameters. Briefly justify your results, and comment on what helps/what doesn't help with lowering the loss, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76y0FAruvXMQ"
   },
   "source": [
    "# Q3. (10 points) - Principal Component Analysis\n",
    "\n",
    "**Part 1.** (5 points) Briefly explain the main principle behind the Principal Component Analysis algorithm. In what sense is the selection of the eigenvectors to represent the data an optimal choice.  How do you reconstruct (a noisy version of) the original data from the eigenvectors and the scores?\n",
    "\n",
    "**Part 2**. (5 points) Read this [article](https://erdem.pl/2020/04/t-sne-clearly-explained) on t-SNE, a dimensionality reduction technique for visualization and explain it in your own words in one or two paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly employed in data analysis and machine learning. Its fundamental principle revolves around transforming data into a fresh coordinate system where each axis, represented by an eigenvector, encapsulates a specific source of variation. The selection of these eigenvectors is considered optimal because it prioritizes the most significant sources of data variance.\n",
    "\n",
    "Here's a step-by-step breakdown:\n",
    "\n",
    "1. **Centering Data**: Initially, the data is centered by subtracting the mean of each feature from every data point. This step ensures that the data's average value along each dimension becomes zero.\n",
    "\n",
    "2. **Covariance Matrix**: The next step involves computing the covariance matrix of the centered data. This matrix unveils the relationships between different features and how they co-vary.\n",
    "\n",
    "3. **Eigendecomposition**: The covariance matrix undergoes eigendecomposition, resulting in the identification of eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Selecting Eigenvectors**: These eigenvectors essentially serve as the primary axes of variation in the data, known as principal components. The corresponding eigenvalues quantify the variance explained by each eigenvector. Typically, these eigenvectors are ordered by eigenvalue magnitude in descending order. This arrangement allows for the selection of the top k eigenvectors, preserving a desired amount of data variance (facilitating dimensionality reduction).\n",
    "\n",
    "5. **Reconstruction**: To reconstruct the original data or a version of it with noise, you leverage the chosen eigenvectors (principal components) and the scores. Scores denote the coordinates of data points within the new coordinate system established by the selected eigenvectors. The process of data point reconstruction entails a linear combination of these eigenvectors, where the scores act as the weighting factors. The more eigenvectors used, the closer the reconstructed data aligns with the original data.\n",
    "\n",
    "PCA's effectiveness lies in the optimal choice of eigenvectors because they maximize the capture of variance along each principal component. The first principal component captures the most dominant source of data variation, the second captures the second most influential source, and so forth. By selecting the top eigenvectors, you can effectively reduce data dimensionality while retaining the most critical information. PCA simplifies data preprocessing and enhances visualization by focusing on the most substantial sources of data variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE, short for t-distributed Stochastic Neighbor Embedding, serves as a robust dimensionality reduction technique tailored for high-dimensional data analysis. Unlike linear methods like Principal Component Analysis (PCA), t-SNE focuses on preserving the inherent relationships between data points. It accomplishes this by constructing probability distributions for pairs of data points, both in the original high-dimensional space and a lower-dimensional space, typically 2D or 3D. These probability distributions are established using Gaussian probability functions, and the algorithm's objective is to ensure that similar data points have high probabilities of being neighbors while minimizing the probabilities for dissimilar points. Through an iterative optimization process, t-SNE discovers a lower-dimensional representation of the data that effectively captures its underlying structure, making it particularly valuable for visualizing clusters or patterns within intricate datasets.\n",
    "\n",
    "Nonetheless, it is essential to acknowledge certain limitations of t-SNE. It may not be suitable for dimensionality reduction within machine learning models due to its non-deterministic nature, resulting in potentially different outcomes with each execution. Additionally, t-SNE can be computationally intensive and might not be well-suited for handling extremely large datasets. Nevertheless, it continues to be a favored choice for data visualization and exploration, enabling researchers and data scientists to gain valuable insights into the complex structures of high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7czWThSxqLHZ"
   },
   "source": [
    "# Q4. (20 points) - Principal Component Analysis\n",
    "\n",
    "In this problem we will be applying PCA and T-SNE on the Superconductivity Dataset. More details on the dataset is present [here](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data#). The goal here is to predict the critical temperature of a superconductor based on the features extracted.\n",
    "\n",
    "First use Principal Component Analysis (PCA) to solve this problem.  \n",
    "\n",
    "* **Part 1. (5 points)** Perform the following steps to prepare the dataset:\n",
    "    * Load the dataset from the \"Q4data.csv\" file provided as a dataframe df.\n",
    "\n",
    "    * Select the **'critical_temp'** column as the target column and the rest of the columns from the dataframe df as X.\n",
    "\n",
    "    * Split the dataset into train and test set with 35% data in test set and random_state = 42\n",
    "\n",
    "    * Perform [Standard Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) on the dataset. Remember that when we have training and testing data, we fit preprocessing parameters on training data and apply them to all testing data. You should scale only the features (independent variables), not the target variable y.\n",
    "    \n",
    "    `Note: X should have 81 features.`\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "2qKTTk2I6Q2i",
    "outputId": "d5cd566c-117b-43e5-9dda-431157e6d653"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-89e9bfce-6254-4c43-9f22-291baddc6919\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-89e9bfce-6254-4c43-9f22-291baddc6919\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Q4data.csv to Q4data.csv\n"
     ]
    }
   ],
   "source": [
    "# Only use this code block if you are using Google Colab.\n",
    "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
    "from google.colab import files\n",
    "\n",
    "## It will prompt you to select a local file. Click on ‚ÄúChoose Files‚Äù then select and upload the file.\n",
    "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_YcaHt-tnLC"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import os, sys, re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"Q4data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQ_vteTAtqj-"
   },
   "outputs": [],
   "source": [
    "y = df[\"critical_temp\"]\n",
    "X = df.drop(columns=[\"critical_temp\"])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "### START CODE ###\n",
    "### Scale the dataset\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIhjMtKvt-c-"
   },
   "source": [
    "* **Part 2 (5 points)** Use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and reduce the dimension of X_train to the following number of components: `[3,20,40,60,81]`. For each of the five datasets, print the cumulative variance explained by the principal components`N = [3,20,40,60,81]`.(i.e. what percentage of variance in the original dataset is explained if we transform the dataset to have 3,20,40,60 and 81 principal components respectively).\n",
    "\n",
    "  `Note : PCA should be fit on X_train and the components thus learnt should be later used to transform X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_RBsvNGuB-f"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "nums = [3,20,40,60,81]\n",
    "res = []\n",
    "for num in nums:\n",
    "    ### START CODE ###\n",
    "    ## Fit PCA\n",
    "    ### END CODE ###\n",
    "\n",
    "    ### START CODE ###\n",
    "    ## Transform Data\n",
    "    ### END CODE ###\n",
    "\n",
    "    ### START CODE ###\n",
    "    ## Compute explained variance\n",
    "    ### END CODE ###\n",
    "\n",
    "    print(\"Cumulative variance explained by {} components is {}\".format(num,var[num-1])) #cumulative sum of variance explained with [n] features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msXHrOduuD7U"
   },
   "source": [
    "* **Part 3. (5 points)** Plot the cumulative variance explained by the principal components using the training data. The plot should display the number of components on X-axis and the cumulative explained variance on the y-axis. What do you understand from the plot obtained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-eBOo9dt-6v"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "## Plot the explained variance vs number of components\n",
    "### END CODE ###\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqXkjgDjuIRn"
   },
   "source": [
    "* **Part 4. (5 points)** For each of the reduced dataset, obtained in part 2.2, fit a linear regression model on the train data and show how adjusted $R^2$ varies as a function of # of components.(There will be a total of 5 ${R^2}$ score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvUD1cYttsnd"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "nums = [3,20,40,60,81]\n",
    "res = []\n",
    "for num in nums:\n",
    "\n",
    "    ### START CODE ###\n",
    "    ## Fit PCA components\n",
    "    ### END CODE ###\n",
    "\n",
    "\n",
    "    ### START CODE ###\n",
    "    ## Transform train and test data\n",
    "    ### END CODE ###\n",
    "\n",
    "    ### START CODE ###\n",
    "    ## Compute explained variance\n",
    "    ### END CODE ###\n",
    "\n",
    "    ### START CODE ###\n",
    "    ## Fit LR and compute R-square and adjusted R-squared\n",
    "    ### END CODE ###\n",
    "\n",
    "    adjusted_r_squared = 1 - (1-r_squared)*(len(Y_test)-1)/(len(Y_test)-X_test_new.shape[1]-1)\n",
    "    print(\"Adjusted R^2\",adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79S5IsAIIZ9A"
   },
   "source": [
    "# Q5. (10 points) PCA vs T-SNE\n",
    "* **Part 1.** **(3 points)** Now apply T-SNE to the dataset given above in Q4. You are required to carry out the following tasks:\n",
    "\n",
    "\n",
    "\n",
    "1.   Initialize a t-SNE model with number of dimensions = 3, perplexity = 300, number of iterations = 300 and random state = 42\n",
    "2.   Apply the t-SNE model to the training dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-PgOjrK7GDw"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "### START CODE ###\n",
    "## Initialize t-SNE model\n",
    "### END CODE ###\n",
    "\n",
    "### START CODE ###\n",
    "## Fit and transform the data\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2WZHiIcLB1f"
   },
   "source": [
    "* **Part 2.** (3 points) For this part use a small subset of 500 samples of the training dataset and plot the first three t-SNE components similar to the PCA implementation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "L52XKYoT-mXl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tB-kPi1G1aA"
   },
   "source": [
    "* **Part 3. (4 points)** Now we will plot the PCA and t-SNE projections of the data and compare the plots side-by-side to see the difference in scatters created by the two methods. You can use first 1000 data points for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2-xyBKnIARW"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))  # Adjust the figure size as needed\n",
    "\n",
    "# First subplot (left)\n",
    "\n",
    "### START CODE ###\n",
    "### Obtain components from PCA\n",
    "### END CODE ###\n",
    "\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, select the first subplot\n",
    "plt.title('PCA')\n",
    "\n",
    "### START CODE ###\n",
    "### scatter plot for PCA\n",
    "### END CODE ###\n",
    "\n",
    "\n",
    "# Second subplot (right)\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, select the second subplot\n",
    "\n",
    "### START CODE ###\n",
    "### scatter plot for t-SNE\n",
    "### END CODE ###\n",
    "\n",
    "plt.title('T-SNE')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
